{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f26510f1",
   "metadata": {},
   "source": [
    "**Diziden diziye (Seq2Seq) modeller**, bir giriş dizisini alıp bir çıkış dizisi üretebilen yapay sinir ağı modelleridir. Bu modeller genellikle dil çevirisi, metin özeti oluşturma ve konuşma tanıma gibi doğal dil işleme (NLP) problemlerinde kullanılır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66319421",
   "metadata": {},
   "source": [
    "**Temel Bileşenler:**\n",
    "\n",
    "* **Encoder**: Giriş dizisini kodlayan ve gizli bir vektöre dönüştüren model bileşeni.\n",
    "* **Decoder**: Gizli vektörü çözerek istenen çıkış dizisini üreten model bileşeni.\n",
    "* **Attention Mekanizması (opsiyonel)**: Uzun dizilerle çalışırken modelin belirli zaman adımlarında odaklanmasını sağlar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd043265",
   "metadata": {},
   "source": [
    "# RNN (Recurrent Neural Networks)\n",
    "\n",
    "Tekrar eden bağlantılar içerir ve sıralı verilerle çalışır.\n",
    "Uzun dizilerde gradyan patlaması veya kaybolması problemi yaşar.\n",
    "\n",
    "**Matematiksel Gösterim:**\n",
    "\n",
    "$$h_t = \\sigma(W_h \\cdot h_{t-1} + W_x \\cdot x_t + b)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673effef",
   "metadata": {},
   "source": [
    "![Recurrent Neural Network](pictures/RNN.jpeg \"Recurrent Neural Network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155cd617",
   "metadata": {},
   "source": [
    "# LSTM (Long Short-Term Memory)\n",
    "\n",
    "Bellek hücreleri ve kapılar (gate) kullanarak uzun süreli bağımlılıkları öğrenir.\n",
    "\n",
    "* Kapılar:\n",
    "* Forget Gate: Önceki bilgiyi unutur.\n",
    "* Input Gate: Yeni bilgiyi ekler.\n",
    "* Output Gate: Çıkışı üretir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0745ead",
   "metadata": {},
   "source": [
    "**Matematiksel Gösterim:**\n",
    "    \n",
    "1. **Forget Gate:**\n",
    "\n",
    "   $$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n",
    "\n",
    "2. **Input Gate:**\n",
    "\n",
    "   $$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n",
    "\n",
    "3. **Candidate Memory Cell:**\n",
    "\n",
    "   $$\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$$\n",
    "\n",
    "4. **Memory Cell Update:**\n",
    "\n",
    "   $$C_t = f_t \\ast C_{t-1} + i_t \\ast \\tilde{C}_t$$\n",
    "\n",
    "5. **Output Gate:**\n",
    "\n",
    "   $$h_t = o_t \\ast \\tanh(C_t)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4f05aa",
   "metadata": {},
   "source": [
    "![Long Short-Term Memory](pictures/LSTM.jpeg \"Long Short-Term Memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ecb6d0",
   "metadata": {},
   "source": [
    "# GRU (Gated Recurrent Unit)\n",
    "\n",
    "LSTM’nin daha sade bir versiyonudur. Daha az parametre içerir.\n",
    "\n",
    "**Kapılar:**\n",
    "\n",
    "**Reset Gate**: Önceki bilgiyi sıfırlar.\n",
    "\n",
    "**Update Gate**: Mevcut bilgiyi günceller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2274896",
   "metadata": {},
   "source": [
    "Bir Seq2Seq Modeli Oluşturma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1584b9e6-1f52-447f-968a-20423625ddf0",
   "metadata": {},
   "source": [
    "Basit bir seq2seq modeli oluşturma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5981810f-baf2-4131-93e2-0fe7624ad94f",
   "metadata": {},
   "source": [
    "Başlamadan önce bir conda environment kuralım ve paket yüklemelerini bu environment içinde yapalım.\n",
    "\n",
    "Anaconda Prompt'u açınız ve sırasıyla şu komutları çalıştırınız:\n",
    "\n",
    "* conda create -n tensorflow_env_3-9 python=3.9\n",
    "* conda activate tensorflow_env_3-9\n",
    "* conda install tensorflow numpy scipy\n",
    "* pip install notebook ipykernel\n",
    "\n",
    "Şimdi python dosyanızın bulunduğu konuma giderek jupyter notebook'u çalıştırınız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff78afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, GRU, Dense\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None, 128))  # 128 boyutlu giriş\n",
    "encoder = LSTM(256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None, 128))\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(128, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Rastgele veri oluşturma\n",
    "encoder_input_data = np.random.rand(100, 10, 128)  # 100 örnek, her biri 10 zaman adımı, 128 boyutlu vektör\n",
    "decoder_input_data = np.random.rand(100, 10, 128)\n",
    "decoder_target_data = np.random.rand(100, 10, 128)\n",
    "\n",
    "# Modeli eğitme\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=32, epochs=10)\n",
    "\n",
    "\n",
    "# Encoder Modeli\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder Modeli\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Çıkış Üretimi\n",
    "states_value = encoder_model.predict(encoder_input_data[:1])\n",
    "decoder_input = np.zeros((1, 1, 128))  # Başlangıç sembolü\n",
    "decoded_sequence = []\n",
    "\n",
    "for _ in range(10):  # Maksimum 10 zaman adımı\n",
    "    output_tokens, h, c = decoder_model.predict([decoder_input] + states_value)\n",
    "    decoded_sequence.append(output_tokens)\n",
    "    decoder_input = output_tokens  # Bir sonraki zaman adımının girişi\n",
    "    states_value = [h, c]\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cebb887-b83f-481c-8561-d7121f25bd27",
   "metadata": {},
   "source": [
    "Sentiment Analiz Uygulaması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c60d0b-47f0-426a-8cef-9b9d4bf693fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas==1.5.3\n",
    "!pip install scikit-learn==1.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1463d2f-43cc-4b57-97a1-30ca219099fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# 1. Veri Yükleme\n",
    "file_path = \"dosyalar/twitter_training.csv\"  # CSV dosyasının yolu\n",
    "data = pd.read_csv(file_path, header=None, names=[\"ID\", \"Category\", \"Sentiment\", \"Text\"])\n",
    "\n",
    "# 2. Veriyi İşleme\n",
    "# Sadece metin (Text) ve duygu (Sentiment) sütunlarını alıyoruz\n",
    "texts = data[\"Text\"].values\n",
    "labels = data[\"Sentiment\"].values\n",
    "\n",
    "# Tüm metinlerin string formatında olduğundan emin olalım\n",
    "texts = [str(text) if not isinstance(text, str) else text for text in texts]\n",
    "\n",
    "# Sentiment'i sayısal değerlere dönüştürme (Positive -> 1, Negative -> 0, Neutral -> 2)\n",
    "label_dict = {\"Positive\": 1, \"Negative\": 0, \"Neutral\": 2}\n",
    "labels = np.array([label_dict.get(label, -1) for label in labels])\n",
    "\n",
    "# Geçersiz etiketleri temizleme\n",
    "valid_labels = set(label_dict.values())  # Geçerli etiketler: {0, 1, 2}\n",
    "filtered_indices = [i for i, label in enumerate(labels) if label in valid_labels]\n",
    "texts = [texts[i] for i in filtered_indices]\n",
    "labels = [labels[i] for i in filtered_indices]\n",
    "\n",
    "# 3. Metni Sayısal Hale Getirme (Tokenization)\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Metinleri dizilere dönüştürme\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Dizileri sabit uzunlukta padding ile doldurma\n",
    "max_length = max(len(seq) for seq in sequences)  # En uzun diziyi referans al\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding=\"post\")\n",
    "\n",
    "# 4. Eğitim ve Test Verilerini Ayırma\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Etiketleri numpy array'e dönüştürme\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# 5. Model Oluşturma\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=64, input_length=max_length),  # Embedding katmanı\n",
    "    LSTM(128, return_sequences=False),  # LSTM katmanı\n",
    "    Dropout(0.5),  # Overfitting'i önlemek için dropout\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dense(3, activation=\"softmax\")  # Çok sınıflı sınıflandırma için softmax\n",
    "])\n",
    "\n",
    "# Modeli derleme\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "# 6. Modeli Eğitme\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test)\n",
    ")\n",
    "\n",
    "# 7. Modelin Test Edilmesi\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# 8. Yeni Metinlerin Tahmini\n",
    "new_texts = [\"I will destroy you in Borderlands!\", \"I am enjoying Borderlands 2.\"]\n",
    "new_sequences = tokenizer.texts_to_sequences(new_texts)\n",
    "new_padded_sequences = pad_sequences(new_sequences, maxlen=max_length, padding=\"post\")\n",
    "predictions = model.predict(new_padded_sequences)\n",
    "\n",
    "for text, pred in zip(new_texts, predictions):\n",
    "    sentiment = [\"Negative\", \"Positive\", \"Neutral\"][np.argmax(pred)]\n",
    "    print(f\"Text: {text} -> Sentiment: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f566e64c-bc11-4080-884c-d3f0e8e0816a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818a26a3-f8a9-4693-8fd8-62b8565c2588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# 1. Önişleme Fonksiyonları\n",
    "def extract_text_from_xml(file_path):\n",
    "    \"\"\"XML dosyasından metni çıkarır.\"\"\"\n",
    "    try:\n",
    "        import xml.etree.ElementTree as ET\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "        text_field = root.find(\".//field[@name='text']\")\n",
    "        if text_field is not None:\n",
    "            return text_field.text.strip()\n",
    "        else:\n",
    "            return \"\"\n",
    "    except ET.ParseError as e:\n",
    "        print(f\"Hata: {file_path} XML dosyası düzgün değil. Hata: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Metin önişleme işlemleri.\"\"\"\n",
    "    text = text.lower()  # Küçük harfe çevirme\n",
    "    text = re.sub(r'\\d+', '', text)  # Sayıları kaldırma\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Noktalama işaretlerini kaldırma\n",
    "    return text\n",
    "\n",
    "def load_documents_and_labels(base_dir):\n",
    "    \"\"\"Belge ve etiketleri klasör yapısından yükler.\"\"\"\n",
    "    categories = os.listdir(base_dir)\n",
    "    documents = []\n",
    "    labels = []\n",
    "    for category in categories:\n",
    "        category_path = os.path.join(base_dir, category)\n",
    "        if os.path.isdir(category_path):\n",
    "            for file_name in os.listdir(category_path):\n",
    "                if file_name.endswith(\".xml\"):\n",
    "                    file_path = os.path.join(category_path, file_name)\n",
    "                    text = extract_text_from_xml(file_path)\n",
    "                    processed_text = preprocess_text(text)\n",
    "                    documents.append(processed_text)\n",
    "                    labels.append(category)\n",
    "    return documents, labels\n",
    "\n",
    "# 2. Kelime Vektörlerini Yükleme\n",
    "def load_custom_word_vectors(file_path):\n",
    "    \"\"\"Kelime vektörlerini dosyadan yükler.\"\"\"\n",
    "    word_vectors = {}\n",
    "    vector_size = None\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split('\\t')\n",
    "            word = parts[0]\n",
    "            vector = np.array([float(x) for x in parts[1:]])\n",
    "            if vector_size is None:\n",
    "                vector_size = len(vector)\n",
    "            if len(vector) == vector_size:\n",
    "                word_vectors[word] = vector\n",
    "    return word_vectors\n",
    "\n",
    "def create_embedding_matrix(word_vectors, word_index, vector_size):\n",
    "    \"\"\"Embedding matrisi oluşturur.\"\"\"\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, vector_size))\n",
    "    for word, i in word_index.items():\n",
    "        if word in word_vectors:\n",
    "            embedding_matrix[i] = word_vectors[word]\n",
    "    return embedding_matrix\n",
    "\n",
    "# 3. Veri Yükleme\n",
    "base_dir = 'dosyalar/Milliyet-Collection-Mini'\n",
    "documents, labels = load_documents_and_labels(base_dir)\n",
    "\n",
    "# 4. Etiketleri Kodlama\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(labels)\n",
    "\n",
    "# 5. Metinleri Tokenize Etme\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(documents)\n",
    "X = tokenizer.texts_to_sequences(documents)\n",
    "max_sequence_length = max(len(seq) for seq in X)\n",
    "X = pad_sequences(X, maxlen=max_sequence_length)\n",
    "\n",
    "# 6. Kelime Vektörlerini Yükleme ve Embedding Matrisi Oluşturma\n",
    "word_vectors = load_custom_word_vectors('dosyalar/cbow_word_vectors.txt')  # Kelime vektörleri dosyası\n",
    "vector_size = len(next(iter(word_vectors.values())))  # İlk vektörün boyutunu al\n",
    "embedding_matrix = create_embedding_matrix(word_vectors, tokenizer.word_index, vector_size)\n",
    "\n",
    "# 7. LSTM Modeli\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=len(tokenizer.word_index) + 1, \n",
    "              output_dim=vector_size, \n",
    "              weights=[embedding_matrix], \n",
    "              input_length=max_sequence_length, \n",
    "              trainable=False),  # Önceden eğitilmiş vektörleri sabitle\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(np.unique(y)), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 8. Eğitim ve Test Setlerine Bölme\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 9. Modeli Eğitme\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n",
    "\n",
    "# 10. Modeli Değerlendirme\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Doğruluk (Accuracy): {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68454025-d31b-4679-871e-e06d0288c38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category(text, model, tokenizer, label_encoder, max_sequence_length):\n",
    "    # 1. Önişleme\n",
    "    processed_text = preprocess_text(text)\n",
    "    \n",
    "    # 2. Sekansa Dönüştürme\n",
    "    sequence = tokenizer.texts_to_sequences([processed_text])  # Tek metin için\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_sequence_length)\n",
    "    \n",
    "    # 3. Tahmin Yapma\n",
    "    prediction = model.predict(padded_sequence)\n",
    "    predicted_label_index = np.argmax(prediction, axis=1)  # En yüksek olasılığa sahip sınıfı al\n",
    "    \n",
    "    # 4. Tahmini Etiketle Dönüştürme\n",
    "    predicted_label = label_encoder.inverse_transform(predicted_label_index)\n",
    "    \n",
    "    return predicted_label[0]\n",
    "\n",
    "# Örnek metin\n",
    "example_text = \"Merkez bankası para arzını artıracak.\"\n",
    "\n",
    "# Kategori tahmini\n",
    "predicted_category = predict_category(example_text, model, tokenizer, label_encoder, max_sequence_length)\n",
    "print(f\"Tahmin edilen kategori: {predicted_category}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
